services:
  llama-cpp-server-cpu:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: llama-cpp-server-cpu
    ports:
      - "8686:8686"
    command:
      - -m
      - /models/Model-7.6B-Q4_0.gguf
      - --mmproj
      - /models/mmproj-model-f16.gguf
      - --port
      - "8686"
      - --host
      - 0.0.0.0
      - -t
      - "32"
