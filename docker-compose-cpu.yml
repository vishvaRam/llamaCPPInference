services:
  llama-cpp-server:
    image: ghcr.io/ggml-org/llama.cpp:server 
    container_name: llama-cpp-server
    ports:
      - "8000:8000"
    volumes:
      - D:\Data Science\JUNE_2025\LlamaCPP\models:/models
    command:
      - -m
      - /models/Model-7.6B-Q8_0.gguf
      - --no-mmproj-offload
      # - --mmproj
      # - /models/mmproj-model-f16.gguf
      - --port
      - "8000"
      - --host
      - 0.0.0.0
      - -t
      - "16"
